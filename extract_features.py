# -*- coding: utf-8 -*-
"""02b

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kw45_fnquCWd76cQCE2wTCLU1qWZJ7R4
"""

# install
!pip install -q cadl
!apt-get install -q imagemagick

# imports
import glob
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import json
import seaborn as sns
import requests
from cadl import utils, gif
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from skimage.transform import resize
from sklearn.decomposition import IncrementalPCA as PCA
from sklearn.manifold import TSNE
from umap import UMAP
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment
from google.colab import drive

# file stuff
drive.mount('/content/drive')
files = glob.glob("/content/drive/My Drive/UCLA/F20-171/cyanotypes/tiff/all/cropped/*-col2*.jpg")
len(files)

cyanotypes = []
for cyano in files:
  cyanotype = plt.imread(cyano)
  #cyanotype = cyanotype[..., :3]
  cyanotype = utils.imcrop_tosquare(cyanotype)
  cyanotype = resize(cyanotype, (256, 256))
  cyanotypes.append(cyanotype)

cyanotypes_np = np.array(cyanotypes)

mean_img = np.mean(cyanotypes_np, axis=0)
std_img = np.std(cyanotypes_np, axis=0)

plt.imshow(mean_img)
plt.figure()
plt.imshow(std_img)

# add -3 and +3 standard deviations to mean image
model = [mean_img + i * std_img for i in np.linspace(-3, 3, 100)]

# normalize each image from 0 to 1
model = [(cyano - np.min(cyano)) / np.ptp(cyano) for cyano in model]

# make gif
gif.build_gif(model)

# make montage
montage_model = [model[cyano] for cyano in np.linspace(0, 99, 16).astype(int)]
montage = utils.montage(montage_model)

# plot montage
plt.figure(figsize=(10,10))
plt.imshow(montage)

# load Inception model
net = tf.keras.applications.InceptionV3()
net.trainable = False

# get activations of layers
names = ['avg_pool', 'predictions']
layers = [net.get_layer(name).output for name in names]

# create feature extraction model
model = tf.keras.Model(inputs=net.input, outputs=layers)

# sanity check
cyanotype = cyanotypes[0]
plt.imshow(cyanotype)
print(cyanotype.dtype, cyanotype.min(), cyanotype.max(), cyanotype.shape)

# load Inception model
net = tf.keras.applications.InceptionV3()
net.trainable = False

# get activations of layers
names = ['avg_pool', 'predictions']
layers = [net.get_layer(name).output for name in names]

# create feature extraction model
model = tf.keras.Model(inputs=net.input, outputs=layers)

# download labels
response = requests.get("https://git.io/JJkYN")
labels = response.text.split("\n")
print(labels)

# get activations for all cyanotypes
activations = model(
    tf.keras.applications.inception_v3.preprocess_input(
        (cyanotypes_np * 255)
    )
)

# montage all cyanotypes
montage = utils.montage(cyanotypes_np)
plt.imshow(montage)

# ask for most confident predictions and convert to numpy array
preds = activations[-1].numpy()
[labels[cyano] for cyano in preds.argmax(axis=1)]

# extract features
features = activations[0].numpy()
features.shape
features

# get features for each network
features_2d_pca = PCA(n_components=2).fit_transform(X=features)
features_2d_tsne = TSNE(n_components=2).fit_transform(X=features)
features_2d_umap = UMAP(n_components=2).fit_transform(X=features)

fig, axs = plt.subplots(1, 3, figsize=(15, 5))
axs[0].scatter(features_2d_pca[:, 0], features_2d_pca[:, 1])
axs[0].set_title("PCA")
axs[1].scatter(features_2d_tsne[:, 0], features_2d_tsne[:, 1])
axs[1].set_title("TSNE")
axs[2].scatter(features_2d_umap[:, 0], features_2d_umap[:, 1])
axs[2].set_title("UMAP")

zoom = 0.15
features_2d = features_2d_tsne
# features_2d = features_2d_pca

fig, ax = plt.subplots(figsize=(16, 16))
for i, img in enumerate(cyanotypes_np):
  img_box = OffsetImage(img, zoom=zoom, alpha=0.8)
  xy = (features_2d[i][0], features_2d[i][1])
  ab = AnnotationBbox(img_box, xy, xycoords='data')
  ax.add_artist(ab)
  ax.update_datalim([xy])
  ax.autoscale()

# number of images
n_imgs = len(cyanotypes_np)

# create a grid that spans the range of values of our projected 2D features
indexes = np.linspace(np.min(features_2d) * 2.0, np.max(features_2d) * 2.0, int(np.ceil(np.sqrt(n_imgs))))
xs, ys = np.meshgrid(indexes, indexes)

# grid could be larger than necessary, slice it to the number of images
grid = np.dstack((ys, xs)).reshape(-1, 2)[:n_imgs,:]

# visualize assignment
fig, axs = plt.subplots(1, 2, figsize=(8, 3))
axs[0].scatter(features_2d[:, 0], features_2d[:, 1], edgecolors='none', marker='o', s=2)
axs[0].set_title('TSNE Embedding')
axs[1].scatter(grid[:,0], grid[:,1],
               edgecolors='none', marker='o', s=2)
axs[1].set_title('Idealized Grid')

# calculate new indexes
cost = cdist(grid[:, :], features_2d[:, :], 'sqeuclidean')
indexes = linear_sum_assignment(cost)

plt.figure(figsize=(5, 5))
for i in range(n_imgs):
  plt.plot([features_2d[indexes[1][i], 0], grid[i, 0]],
           [features_2d[indexes[1][i], 1], grid[i, 1]], 'r')

sorted = [cyanotypes_np[cyano] for cyano in indexes[1]]
montage = utils.montage(sorted)
plt.figure(figsize=(10, 10))
plt.imshow(montage)

#x_features = features_2d_pca[:,0]
#y_features = features_2d_pca[:,1]
x_features = features_2d_tsne[:,0]
y_features = features_2d_tsne[:,1]

x_min, x_max = x_features.min()-1, x_features.max()+1
y_min, y_max = y_features.min()-1, y_features.max()+1

x, y = np.meshgrid(np.arange(x_min, x_max, 0.2),
                   np.arange(y_min, y_max, 0.2))

df = pd.DataFrame(features_2d_pca)

fig, ax = plt.subplots()
#sns.kdeplot(x=features_2d_pca[:,0], y=features_2d_pca[:,1], cmap="PRGn", ax=ax)
#sns.scatterplot(x=features_2d_pca[:,0], y=features_2d_pca[:,1], color="red", ax=ax)
sns.kdeplot(x=features_2d_tsne[:,0], y=features_2d_tsne[:,1], cmap="PRGn", ax=ax)
sns.scatterplot(x=features_2d_tsne[:,0], y=features_2d_tsne[:,1], color="red", ax=ax)
plt.show()